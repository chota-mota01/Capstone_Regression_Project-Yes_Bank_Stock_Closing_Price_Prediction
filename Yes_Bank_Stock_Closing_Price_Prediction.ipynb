{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Y3lxredqlCYt",
        "dauF4eBmngu3",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "KSlN3yHqYklG",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "bamQiAODYuh1",
        "OH-pJp9IphqM",
        "PIIx-8_IphqN",
        "jj7wYXLtphqO",
        "XTJYCpLIPU6_",
        "YJ55k-q6phqO",
        "U2RJ9gkRphqQ",
        "x-EpHcCOp1ci",
        "NC_X3p0fY2L0",
        "q29F0dvdveiT",
        "g-ATYxFrGrvw",
        "Nff-vKELpZyI",
        "xiyOF9F70UgQ",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "TNVZ9zx19K6k",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chota-mota01/Capstone_Regression_Project-Yes_Bank_Stock_Closing_Price_Prediction/blob/main/Yes_Bank_Stock_Closing_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Closing Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank, a leading institution in the Indian financial sector, garnered attention following the 2018 fraud case involving its former CEO, Rana Kapoor. The project was conducted individually, and after analysing the dataset, it was discovered that it contained 185 rows and 5 columns. This dataset comprises monthly stock prices of Yes Bank since its inception, including key metrics such as closing, opening, highest, and lowest prices for each month. The main objective of the project was to forecast the monthly closing price of Yes Bank's stock.\n",
        "\n",
        "To avoid affecting the original data, a copy was made, and specified column was converted to appropriate data type. The analysis of the cleaned data provided valuable insights into stock closing price prediction. The columns of the dataset exhibit positive skewness. The application of log transformation has resulted in a more symmetrical distribution, thereby enhancing the accuracy of predictions. Moreover, the variables exhibit high correlation with each other. The log transformation effectively reduced outliers in the dataset, thereby contributing to a more robust model.\n",
        "\n",
        "For data visualization, the researcher used the seaborn and matplotlib libraries and various types of graphs, such as distplot, line chart, scatter plot, box plot,histogram plot, correlation heatmap, and pair plot. These visualizations helped to simplify complex data and make it more understandable.\n",
        "\n",
        "Four machine learning models were developed to forecast the monthly closing price of Yes Bank's stock.: Linear Regression, Lasso Regression, Ridge Regression, and Random Forest. 'Open', 'High', and 'Low' features were aggregated by taking their mean and engineered additional features using lag values to capture temporal trends and patterns, including potential effects of the fraud case. These models were evaluated using various metrics to gain insights into the data, facilitating better decision-making for improved outcomes. The evaluation metrics used were Mean Absolute Error(MAE),Mean Squared Error(MSE), Root Mean Squared Error(RMSE), R-squared(R2) and Adjusted R-squared. Additionally, cross-validation techniques were employed to estimate optimal values, have provided us with robust tools for making informed decisions regarding stock investment strategies.\n",
        "\n",
        "Random Forest was selected as my final model due to its superior performance in terms of R2 and adjusted R2 values. These metrics indicate that Random Forest is effectively explaining the variance in the target variable while considering all features. Random Forest model attained the lowest Mean Absolute Error(MAE) value i.e. 3.301648.\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) stands as a sophisticated approach in exploring model interpretability within the realm of machine learning. This technique meticulously assesses the impact of each feature on the ultimate prediction, offering valuable insights into the nuanced influence of individual model components on the overall outcome.\n",
        "\n",
        "Moreover, data visualization techniques have played a crucial role in simplifying complex data and facilitating a clearer understanding of stock price trends and patterns. By leveraging these insights, both investors and financial institutions can make data-driven decisions to optimize their investment portfolios and enhance their overall financial performance.\n",
        "\n",
        "Overall, the findings of this analysis can empower stakeholders in the financial sector to adapt and respond effectively to market dynamics, thereby improving their services, mitigating risks, and ultimately, meeting the evolving needs and preferences of investors and customers alike.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes Bank, a prominent institution in the Indian financial sector, gained notoriety following the 2018 fraud case involving its former CEO, Rana Kapoor. This dataset encompasses monthly stock prices of Yes Bank since its establishment, encompassing metrics such as closing, opening, highest, and lowest prices for each month. The focal point revolves around discerning how such high-profile events, like the aforementioned scandal, have influenced the company's stock prices. Can time series models, or other predictive methodologies, effectively capture and predict the fluctuations resulting from such circumstances? The primary goal is to forecast the monthly closing price of Yes Bank's stock, leveraging insights gained from this analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge, RidgeCV\n",
        "from sklearn.linear_model import Lasso, LassoCV\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Importing datetime modules\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Dataset\n",
        "path = '/content/drive/My Drive/Colab Notebooks/data_YesBank_StockPrices.csv'\n",
        "ybank_data = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "MxNUwIf1u8yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# head() method returns first 5 rows of the dataset\n",
        "ybank_data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Last Look\n",
        "# tail() method returns last 5 rows of the dataset\n",
        "ybank_data.tail()"
      ],
      "metadata": {
        "id": "xRnVLnKXvMBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If number is specified, head() returns specified number of first rows\n",
        "ybank_data.head(10)"
      ],
      "metadata": {
        "id": "wdlsNzlOvQUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If number is specified, tail() returns specified number of last rows\n",
        "ybank_data.tail(10)"
      ],
      "metadata": {
        "id": "XSKUng8jvTc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "ybank_data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "ybank_data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns present in the dataset\n",
        "list(ybank_data.columns)"
      ],
      "metadata": {
        "id": "o71jS3JtvelS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "ybank_data.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "ybank_data.isna().sum().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Used isnull().sum method to view null value in each column\n",
        "ybank_data.isnull().sum()"
      ],
      "metadata": {
        "id": "oRM9uy0KvpBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Check Null value by plotting Heatmap\n",
        "from pickle import FALSE\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(ybank_data.isnull(),cbar=FALSE)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given dataset contains the monthly stock prices of Yes Bank since its establishment and includes closing, opening, highest, and lowest prices for each month. The primary goal is to forecast the monthly closing price of the stock.\n",
        "\n",
        "The dataset has 185 rows and 5 columns. The dataset contains 0 missing/null values and 0 duplicate values.\n",
        "\n",
        "Using seaborn library, we have visualized no missing/null values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "ybank_data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "ybank_data.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Date**       **:**  Date of the stock price record\n",
        "* **Open**       **:**  Opening price of the stock\n",
        "* **High**       **:**  Highest price of the stock for the day\n",
        "* **Low**        **:**  Lowest price of the stock for the day\n",
        "* **Close**      **:**  Closing price of the stock for the day"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable\n",
        "\n",
        "for column in ybank_data:\n",
        "  print(ybank_data[column].unique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of Unique Values for each variable.\n",
        "for col in ybank_data:\n",
        "  print(\"Count of unique values in\",col,\"is\",ybank_data[col].nunique(),\".\")"
      ],
      "metadata": {
        "id": "KlmJLBxrwCZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Create copy of the dataset\n",
        "ybank_df = ybank_data.copy()\n",
        "ybank_df.columns"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting data type of date column\n",
        "ybank_df['Date'] = pd.to_datetime(ybank_df['Date'].apply(lambda x: datetime.strptime(x,'%b-%y')))"
      ],
      "metadata": {
        "id": "DFprjbxzwKi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ybank_df.info()"
      ],
      "metadata": {
        "id": "xKfZ4Yp2wNqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking a Numerical Feature from Data\n",
        "num_fea = ybank_df.describe().columns\n",
        "num_fea"
      ],
      "metadata": {
        "id": "RP2Ynz9DwBsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before manipulation of the data, I created a copy of the yes bank dataset because of which the changes made in the duplicate dataset won't affect the original dataset.\n",
        "\n",
        "After creating copy of the dataset, I changed the datatype of the 'Date' variable to datetime. The remaining variables are numerical. The unique values in 'Date', 'Open', 'High', 'Low' and 'Close' is 185, 183, 184, 183 and 185 respectively . We also observed that the mean is higher in numerical columns .i.e.there is possibility of skewness. The given dataset contains no null and duplicate values.  \n",
        "\n",
        "The dataset is good for better visualization.  "
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Distplot of Dependent Variable"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Check Distribution of the dependent variable 'Close'\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.distplot(ybank_df['Close'],color=\"b\")\n",
        "plt.gca().set_xlabel('Closing Price')\n",
        "plt.gca().set_title('Distribution of Dependent Variable')\n",
        "plt.axvline(ybank_df['Close'].mean(),color='brown')\n",
        "plt.axvline(ybank_df['Close'].median(),color='yellow',linestyle='dashed')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart was choosed because distplot, short for \"distribution plot,\" represents the distribution of a univariate dataset. It combines a histogram with a kernel density estimate (KDE) plot, providing a visual summary of the distribution of values in the dataset."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight gained from the chart is that the data exhibits positive skewness."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart, we can conclude that the closing price fluctuates over time, especially due to significant events such as the fraud in 2018, the data may exhibit positive skewness. Applying a transformation to the data could lead to a more symmetrical distribution, which can improve the accuracy of predicting the closing price."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Distplot of Dependent Variable with Transformation"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Apply Log transformation on Dependent variable 'Close'\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.distplot(ybank_df['Close'],color=\"b\")\n",
        "plt.gca().set_xlabel('Closing Price')\n",
        "plt.gca().set_title('Distribution of Dependent Variable after Transformation')\n",
        "plt.axvline(np.log10(ybank_df['Close']).mean(),color='brown')\n",
        "plt.axvline(np.log10(ybank_df['Close']).median(),color='yellow',linestyle='dashed')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart was choosed because distplot, short for \"distribution plot,\" represents the distribution of a univariate dataset. It combines a histogram with a kernel density estimate (KDE) plot, providing a visual summary of the distribution of values in the dataset."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log transformation was applied to normalize 'Close' column. After transformation, the data resembles a normal distribution, with the mean and median being nearly identical."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dependent variable 'Close' follows the normal distribution. Developing a good model becomes easier when the data follows a normal distribution."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Lineplot of Closing Price yearly"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Doing Visualisation of Closing Price yearly\n",
        "plt.figure(figsize=(15,5))\n",
        "sns.lineplot(x='Date',y='Close',data=ybank_df)\n",
        "plt.gca().set_xlabel('Date')\n",
        "plt.gca().set_ylabel('Close')\n",
        "plt.gca().set_title('Closing Price Yearly')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart is a graphical representation of an asset's historical price action that connects a series of data points with a continuous line. It shows the relationship between two variables.\n",
        "\n",
        "The specific chart shows the relationship between 'Date' and 'Close'."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that the closing price experienced a substantial decline following the fraud case in 2018."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Boxplot of Data"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "col = list(ybank_df.columns)\n",
        "\n",
        "ybank_df[col].plot(kind='box', title='Boxplot of Data',color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots are a measure of how well the data is distributed in the dataset. These charts display ranges within variables measured. This includes the outliers, the median, the mode, and where the majority of the data points lie in the “box”."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that outliers are present in the dataset."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outliers present in the data may lead to misleading insights and reduce model performance."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Boxplot of Independent Variable"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "ybank_price = np.log(ybank_df[['Open', 'High', 'Low']])\n",
        "plt.figure(figsize=(8, 6))\n",
        "ybank_price.boxplot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots are a measure of how well the data is distributed in the dataset. These charts display ranges within variables measured. This includes the outliers, the median, the mode, and where the majority of the data points lie in the “box”."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log transformation is applied in the following chart. It appears that outliers have been mitigated or reduced."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The absence of outliers in the data contributes to the development of a robust model. However, with a small dataset, it is generally not recommended to entirely eliminate outliers."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Histplot for Distibution of Independent Variables"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "sns.histplot(ybank_df['Open'],kde='True',ax=axes[0],color='skyblue')\n",
        "sns.histplot(ybank_df['High'],kde='True',ax=axes[1],color='red')\n",
        "sns.histplot(ybank_df['Low'],kde='True',ax=axes[2],color='orange')\n",
        "\n",
        "# Adjust layout\n",
        "plt.suptitle('Distibution of Independent Variables')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of visualization commonly used to display the distribution of a univariate dataset. It represents the frequency or count of observations falling within predefined intervals, called bins, by plotting bars whose heights correspond to these counts."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight gained from the chart is that the independent variables in the data exhibits positive skewness (right-skewed)."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Histplot for Distibution of Independent Variables After Transformation"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "sns.histplot(np.log10(ybank_df['Open']),kde='True',ax=axes[0],color='skyblue')\n",
        "sns.histplot(np.log10(ybank_df['High']),kde='True',ax=axes[1],color='red')\n",
        "sns.histplot(np.log10(ybank_df['Low']),kde='True',ax=axes[2],color='orange')\n",
        "\n",
        "# Adjust layout\n",
        "plt.suptitle('Distibution of Transformed Independent Variables')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of visualization commonly used to display the distribution of a univariate dataset. It represents the frequency or count of observations falling within predefined intervals, called bins, by plotting bars whose heights correspond to these counts."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log transformation was applied to normalize the independent columns(Open, High ,Low). After transformation, the data resembles a normal distribution."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(x='Open',y='Close',data=ybank_df)\n",
        "correlation=ybank_df['Open'].corr(ybank_df['Close'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Scatter plot for Open vs Close"
      ],
      "metadata": {
        "id": "XTJYCpLIPU6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(x='Open',y='Close',data=ybank_df)\n",
        "correlation=ybank_df['Open'].corr(ybank_df['Close'])\n",
        "plt.gca().set_xlabel('Open')\n",
        "plt.gca().set_ylabel('Close')\n",
        "plt.gca().set_title('Open vs Close - correlation: ' + str(round((correlation),6)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rkbz1dn4PU7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ezqUuqdsPU7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots are the graphs that represents the relationship between the specified two variables in the dataset. Each member of the dataset gets plotted as a point whose x-y coordinates relates to its values for the two variables.\n",
        "\n",
        "The specific chart tells us the relationship between the opening price and the closing price."
      ],
      "metadata": {
        "id": "qXG59MKVPU7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "T_XL5zgMPU7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that the opening price and closing price are highly correlated."
      ],
      "metadata": {
        "id": "XJxt5XQSPU7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Scatter plot for High vs Close"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(x='High',y='Close',data=ybank_df)\n",
        "correlation=ybank_df['High'].corr(ybank_df['Close'])\n",
        "plt.gca().set_xlabel('High')\n",
        "plt.gca().set_ylabel('Close')\n",
        "plt.gca().set_title('High vs Close - correlation: ' + str(round((correlation),6)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots are the graphs that represents the relationship between the specified two variables in the dataset. Each member of the dataset gets plotted as a point whose x-y coordinates relates to its values for the two variables.\n",
        "\n",
        "The specific chart tells us the relationship between high price and the closing price."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that the high price and closing price are highly correlated."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Scatter plot for Low vs Close"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.scatter(x='Low',y='Close',data=ybank_df)\n",
        "correlation=ybank_df['Low'].corr(ybank_df['Close'])\n",
        "plt.gca().set_xlabel('Low')\n",
        "plt.gca().set_ylabel('Close')\n",
        "plt.gca().set_title('Low vs Close - correlation: ' + str(round((correlation),6)))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots are the graphs that represents the relationship between the specified two variables in the dataset. Each member of the dataset gets plotted as a point whose x-y coordinates relates to its values for the two variables.\n",
        "\n",
        "The specific chart tells us the relationship between the low price and the closing price."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that the low price and closing price are highly correlated."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Distplot for the Data"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "sns.distplot(ybank_df['Open'],kde=True,color='r')\n",
        "sns.distplot(ybank_df['High'],kde=True,color='y')\n",
        "sns.distplot(ybank_df['Low'],kde=True,color='b')\n",
        "sns.distplot(ybank_df['Close'],kde=True,color='m')\n",
        "plt.gca().set_title('Distribution of Variables')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart was choosed because distplot, short for \"distribution plot,\" represents the distribution of a univariate dataset. It combines a histogram with a kernel density estimate (KDE) plot, providing a visual summary of the distribution of values in the dataset."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight derived from the chart is that the variables (open, close, low, high) exhibit positive skewness and are collectively represented."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(20,5))\n",
        "cor = sns.heatmap(ybank_df.corr(),annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heatmaps are a type of plot that visualize the strength of relationships between numerical variables. Correlation plots are used to understand which variables are related to each other and the strength of this relationship.\n",
        "\n",
        "I used the correlation heatmap to find correlation between all the variables along with correlation coefficient."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above correlation heatmap, we can see that there is correlation between all the independent variables. Also, the dependent and independent variables are highly correlated."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(ybank_df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical. Pairplot allows us to plot pairwise relationships between variables within a dataset."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot basically plots entire dataframe. Plots between each column take place in pairplot and a big plot is created to compare overall relationship between each column. Some outliers can be seen in the data. This creates nice visualization and helps us understand the large amount of data in a single figure."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement 1- There is a difference between the closing stock prices from the years 2005 to 2015 and those from 2016 to 2020.\n",
        "\n",
        "Statement 2- The closing prices between months with high opening prices and those in months with low opening prices exhibit a significant difference.\n",
        "\n",
        "Statement 3- The maximum closing price recorded for Yes Bank's stock in the year 2018 exceeds 350 price."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a difference between the closing stock prices from the years 2005 to 2015 and those from 2016 to 2020.\n",
        "\n",
        "H0 - The closing stock prices from the years 2005 to 2015, in comparison to those from 2016 to 2020, exhibit no differences.\n",
        "\n",
        "H1- The closing stock prices from the years 2005 to 2015, in comparison to those from 2016 to 2020, exhibit notable differences.\n",
        "\n",
        "Significance level - 0.05"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting date as index\n",
        "ybank_df.set_index('Date',inplace=True)"
      ],
      "metadata": {
        "id": "Exs8hLe-W4xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Using z test to find p value\n",
        "# Setting apla value to 0.05\n",
        "\n",
        "from scipy import stats\n",
        "alpha=0.05\n",
        "\n",
        "# Divide dataset into two part to find the correct value\n",
        "first_half=ybank_df['Close'][ybank_df.index.year <=2015]\n",
        "second_half=ybank_df['Close'][ybank_df.index.year >=2016]\n",
        "\n",
        "# Finding mean, standard deviation & sample size for the sample\n",
        "mean1 = np.mean(first_half)\n",
        "std1 = np.std(first_half)\n",
        "n1 = len(first_half)\n",
        "mean2 = np.mean(second_half)\n",
        "std2 = np.std(second_half)\n",
        "n2 = len(second_half)\n",
        "\n",
        "# Calculate the standard error difference between the two means\n",
        "standard_error = np.sqrt ((std1**2/n1)+(std2**2/n2))\n",
        "\n",
        "# Calculate Z scure using standard error value\n",
        "z = (mean1-mean2)/standard_error\n",
        "\n",
        "# Calculate the p value (two tailed)\n",
        "p_value = 2*(1-stats.norm.cdf(abs(z)))\n",
        "\n",
        "# Significance value of alpha is already set which is 0.05 so comparing p value with z\n",
        "if p_value < alpha:\n",
        "  print('Reject the Null Hypothesis as it is evident that there is significant difference in mean')\n",
        "else:\n",
        "  print ('Failed to reject Null Hypothesis as it is evident that there is not significant difference in mean')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Z-test is a statistical hypothesis test that is used to compare the mean of a sample to a known population mean or to compare the means of two samples, assuming that the population standard deviation is known. It is commonly used when the sample size is large (typically n > 30) and the population variance is known.\n",
        "\n",
        "Given that the dataset comprises more than 30 records, the Z-test was employed to calculate the p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the dataset contains more than 30 records and the Z-test is used to obtain a p-value, likely performed a Z-test for a mean and standard deviation."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The closing prices between months with high opening prices and those in months with low opening prices exhibit a significant difference.\n",
        "\n",
        "Null Hypothesis (H0): The mean closing prices in months with high opening prices are equal to or lower than the mean closing prices in months with low opening prices.\n",
        "\n",
        "Alternative Hypothesis (HA): The mean closing prices in months with high opening prices are higher than the mean closing prices in months with low opening prices."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# Calculating the mean opening and closing prices for each month\n",
        "ybank_df['Month'] = ybank_df.index.strftime('%Y-%m')\n",
        "monthly_data = ybank_df.groupby('Month').agg({'Open': 'mean', 'Close': 'mean'})\n",
        "\n",
        "# Setting a threshold for high and low opening prices\n",
        "threshold = monthly_data['Open'].median()\n",
        "\n",
        "# Dividing the data into groups low and high opening prices based on threshold value of opening value\n",
        "high_opening_prices = monthly_data[monthly_data['Open'] > threshold]['Close']\n",
        "low_opening_prices = monthly_data[monthly_data['Open'] <= threshold]['Close']\n",
        "\n",
        "# Calculating the mean,standard deviation & size for sample\n",
        "mean_high = np.mean(high_opening_prices)\n",
        "mean_low = np.mean(low_opening_prices)\n",
        "std_high = np.std(high_opening_prices)\n",
        "std_low = np.std(low_opening_prices)\n",
        "n_high = len(high_opening_prices)\n",
        "n_low = len(low_opening_prices)\n",
        "\n",
        "# Calculate the z-statistic\n",
        "z_statistic = (mean_high - mean_low) / np.sqrt((std_high**2 / n_high) + (std_low**2 / n_low))\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 1 - stats.norm.cdf(z_statistic)\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare the p-value with the significance level\n",
        "if p_value < alpha:\n",
        " print(\"Reject the null hypothesis. There is a significant difference in the mean closing prices.\")\n",
        "else:\n",
        " print(\"Fail to reject the null hypothesis. There is no significant difference in the mean closing prices.\")\n",
        "\n",
        "# Using independent t test as well\n",
        "t_statistic,p_value = stats.ttest_ind(low_opening_prices, high_opening_prices)\n",
        "# Setting Significance value alpha\n",
        "alpha=0.05\n",
        "\n",
        "# Compare the p-value with the significance level\n",
        "if p_value < alpha:\n",
        "  print(\"Reject the null hypothesis. There is a significant difference in the mean closing prices.\")\n",
        "else:\n",
        "  print(\"Fail to reject the null hypothesis. There is no significant difference in the mean closing prices.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain P-value, Z-test and t-test are used.\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the population standard deviation is known and the sample size is large, a Z-test is typically used. If the population standard deviation is unknown or the sample size is small, a t-test is more appropriate.\n",
        "\n",
        "Since the data is not normally distributed, skewness is a factor to consider."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The maximum closing price recorded for Yes Bank's stock in the year 2018 exceeds 350 price.\n",
        "\n",
        "Null Hypothesis(H0) - The maximum closing price recorded for Yes Bank's stock in the year 2018 does not exceed 350 price.\n",
        "\n",
        "Alternative Hypothesis(H1) - The maximum closing price recorded for Yes Bank's stock in the year 2018 exceeds 350 price."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "mean = 100\n",
        "\n",
        "# Calculating the mean closing price for year\n",
        "ybank_df['Year'] = ybank_df.index.strftime('%Y')\n",
        "yearly_data = ybank_df.groupby('Year').agg({'Close': 'mean'})\n",
        "\n",
        "# Calculating the mean, standard deviation and size for sample\n",
        "sample_mean=np.mean(ybank_df['Close'][ybank_df['Year']==2018])\n",
        "sample_std=np.std(ybank_df['Close'][ybank_df['Year']==2018])\n",
        "sample_size=ybank_df['Close'][ybank_df['Year']==2018].size\n",
        "\n",
        "# Implementing z test to obtain p value\n",
        "z_stat = (sample_mean-mean) / (sample_std/np.sqrt(sample_size))\n",
        "p_value_new = 1-stats.norm.cdf(z_stat)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Now Comparing the p-value with the significance level\n",
        "if p_value < alpha:\n",
        " print(\"Reject the null hypothesis as there are significant evidence regarding Closing price\")\n",
        "else:\n",
        " print(\"Fail to reject the null hypothesis. There is no significant evidence in the mean closing prices.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain p value, z test is done.\n",
        "The Z-test is a statistical hypothesis test that is used to compare the mean of a sample to a known population mean or to compare the means of two samples, assuming that the population standard deviation is known. It is commonly used when the sample size is large (typically n > 30) and the population variance is known."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the dataset contains more than 30 records and the Z-test is used to obtain a p-value."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "'''There are no missing values in the dataset.'''\n",
        "ybank_df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values in the dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "'''Outliers have been addressed through transformation, eliminating the need for additional outlier treatment.\n",
        "Given the small size of the dataset and the limited number of columns, outliers were not directly removed.'''"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers have been treated through log transformation. Given the small size of the dataset and the limited number of columns, outliers were not directly removed."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "'''There are no categorical columns in this dataset; therefore, this step is skipped.'''"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no categorical columns in this dataset; therefore, this step is skipped."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "'''As the correlation chart indicated strong relationships between all independent variables and the dependent variable, a new feature named\n",
        "\"independent_mean\" is created using the mean of all independent variables.'''\n",
        "\n",
        "ybank_df['independent_mean']= ybank_df[['Open','High','Low']].mean(axis=1).round(2)\n",
        "ybank_df.head()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution for independent_mean variable\n",
        "sns.displot(ybank_df['independent_mean'])"
      ],
      "metadata": {
        "id": "tEC8wWkw2HCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying log transform on indepent_mean variable\n",
        "x = np.log10(ybank_df['independent_mean'])\n",
        "sns.displot(x)"
      ],
      "metadata": {
        "id": "p3mh6DgU2H4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Using lags as additional features\n",
        "for i in range(1, 14):\n",
        "    ybank_df[\"lag_{}\".format(i)] = ybank_df.independent_mean.shift(i)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation using heatmap between features\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.heatmap(ybank_df.corr())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wnnb6gQI2kou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing the variable as independent and dependent for further transformation\n",
        "y_depend = ybank_df.dropna().Close.values\n",
        "x_independ = ybank_df.dropna().drop(['Open','High','Low','Month'], axis=1)"
      ],
      "metadata": {
        "id": "_-Ng1LMp2zfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lag is used as additional feature.\n",
        "The lag features provide historical information about the stock's price movements, allowing the model to capture trends and seasonality."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the independent variable(Open,High,Low) are the important features as they are highly correlated with dependent variable(Close)."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Transforming independent variable\n",
        "x_independ['independent_mean']=np.log10(x_independ['independent_mean'])\n",
        "\n",
        "# Transforming dependent variable data\n",
        "Y = np.log10(y_depend)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the independent and dependent variables exhibit skewness, albeit small. To address this, a log transformation was applied."
      ],
      "metadata": {
        "id": "_hVrLC1NW2Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# For Scaling, using StandardScalar\n",
        "scaler= StandardScaler()\n",
        "\n",
        "x_scaled=scaler.fit_transform(x_independ.values)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "The given data appears normally distributed after applying log transformation, so we used 'Standard Scaler' for scaling of data. StandardScaler is a useful preprocessing step that can improve the performance, stability, and interpretability of machine learning models, particularly when working with algorithms that require scaled input features."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given dataset contains only a few features, applying dimensionality reduction may not be necessary, and we can focus on other aspects of model development, such as feature engineering, model selection, and hyperparameter tuning, to optimize predictive performance."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Spliting data into train and test sets using train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, Y, test_size = 0.2, random_state = 1)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The splitting ratio 80:20 is considered here. It's essential to consider the specific characteristics of the dataset and the requirements of the problem at hand when determining the optimal train-test split ratio."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In my assessment, the provided dataset does not exhibit class imbalance."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "'''In my assessment, the provided dataset does not exhibit class imbalance.'''"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 1 - Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A linear regression model is a statistical method used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It assumes a linear relationship between the predictors and the outcome, where the outcome variable is a linear combination of the predictor variables, with each predictor weighted by a coefficient.\n",
        "\n",
        "We implemented linear regression model assuming linear relationship between independent variables (Open, High, Low) and a dependent variable (Close)."
      ],
      "metadata": {
        "id": "LCK-Sxn0rIiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Fit the Algorithm\n",
        "reg= LinearRegression().fit(x_train, y_train)\n",
        "reg.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "id": "Ta9KbnMZN6O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_train_pred= reg.predict(x_train)\n",
        "y_test_pred= reg.predict(x_test)"
      ],
      "metadata": {
        "id": "7PBow79_N2M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training Score : {reg.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {reg.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "2m-kxsxQrgIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Evaluating metrics score\n",
        "MSE  = mean_squared_error(10**(y_test), 10**(y_test_pred))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)"
      ],
      "metadata": {
        "id": "CcGXeI3E5zuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating metric scores for both train and test data\n",
        "print('Metrics for Train Data are as follows:')\n",
        "print('\\n')\n",
        "train_MAE = mean_absolute_error(10**(y_train),(10**y_train_pred))\n",
        "print(\"MAE :\", train_MAE)\n",
        "\n",
        "train_MSE  = mean_squared_error(10**(y_train), 10**(y_train_pred))\n",
        "print(\"MSE :\" , train_MSE)\n",
        "\n",
        "train_RMSE = np.sqrt(train_MSE)\n",
        "print(\"RMSE :\" ,train_RMSE)\n",
        "\n",
        "train_r2 = r2_score(10**(y_train), 10**(y_train_pred))\n",
        "print(\"R2 :\" ,train_r2)\n",
        "\n",
        "train_adj_r2=1-(1-r2_score(10**(y_train), 10**(y_train_pred)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adj_r2)\n",
        "print('\\n')\n",
        "\n",
        "print('Metrics for Test Data are as follows:')\n",
        "print('\\n')\n",
        "\n",
        "MAE = mean_absolute_error(10**(y_test),(10**y_test_pred))\n",
        "print(f\"MAE : {MAE}\")\n",
        "\n",
        "MSE  = mean_squared_error(10**(y_test), 10**(y_test_pred))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test), 10**(y_test_pred))\n",
        "print(\"R2 :\" ,r2)\n",
        "\n",
        "adj_r2=1-(1-r2_score(10**(y_test), 10**(y_test_pred)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adj_r2)\n"
      ],
      "metadata": {
        "id": "wm6wiPnt56uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg_list = {'Train Mean Absolute Error':train_MAE,'Train Mean squared Error' : train_MSE,'Train Root Mean squared Error' : train_RMSE,'Train R2 score' : train_r2,'Train Adjusted R2 score' : train_adj_r2,'Mean Absolute Error':MAE,'Mean squared Error' : MSE,'Root Mean squared Error' : RMSE,'R2 score' : r2,'Adjusted R2 score' : adj_r2 }\n",
        "eva_metrics = pd.DataFrame.from_dict(lin_reg_list, orient='index').reset_index()\n",
        "eva_metrics = eva_metrics.rename(columns={'index':' Evaluation Metric',0:'reg'})\n",
        "eva_metrics"
      ],
      "metadata": {
        "id": "O8OejhmE0n0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test set exhibits lower Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) values compared to the train set. This suggests that the model performs better on the test data than on the training data.\n",
        "\n",
        "The R2 score for the test set is slightly higher than that for the train set, suggesting that the model generalizes well to unseen data.\n",
        "\n",
        "However, the adjusted R2 score for the test set is lower than that for the train set, indicating a potential overfitting issue, where the model may be fitting too closely to the training data.\n",
        "\n",
        "Overall, the model demonstrates good performance on both the train and test sets, with low errors and high R2 scores. However, it's crucial to monitor the adjusted R2 score and be cautious of overfitting when interpreting the results. To mitigate overfitting, regularization techniques such as Ridge regression or Lasso regression can be applied."
      ],
      "metadata": {
        "id": "h7ze6v1UpSZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics=['MAE','MSE','RMSE','R2','Adjusted R2']\n",
        "metrics_score=[MAE,MSE,RMSE,r2,adj_r2]\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(metrics,metrics_score,color='red', linewidth = 3,marker='o', markerfacecolor='green', markersize=12)\n",
        "plt.gca().set_xlabel('Evaluation Metrics')\n",
        "plt.gca().set_ylabel('Metrics Scores')\n",
        "plt.gca().set_title('Visualization of Evaluation Metrics')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing actual and predicted values\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(10**(y_test_pred))\n",
        "plt.plot(np.array(10**(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K4sWTzZP6V6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "parameter = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'copy_X': [True, False],\n",
        "\n",
        "    'positive': [True, False]\n",
        "}\n",
        "\n",
        "# Create the grid search object\n",
        "Lr_gs=GridSearchCV(reg,param_grid=parameter,cv=5,scoring='r2')\n",
        "\n",
        "# Fit the Algorithm\n",
        "Lr_gs.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_test_gs=Lr_gs.predict(x_test)\n",
        "y_pred_train_gs=Lr_gs.predict(x_train)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lr_gs.best_score_"
      ],
      "metadata": {
        "id": "srUN0NMTNZGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing Actual vs Prediction values for Linear Regression\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot with transformation\n",
        "plt.plot(10 ** (y_pred_test_gs))\n",
        "plt.plot(np.array(10 ** (y_test)))\n",
        "plt.legend([\"Predicted\", \"Actual\"])\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sPngIDeX6qFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_reg_list = {'MAE' : MAE,'MSE' : MSE,'RMSE' : RMSE ,'R2 score' : r2,'Adjusted R2 score' : adj_r2}\n",
        "evl_metrics = pd.DataFrame.from_dict(linear_reg_list, orient='index').reset_index()\n",
        "evl_metrics = evl_metrics.rename(columns={'index':'Evaluation Metric',0:'Linear'})\n",
        "evl_metrics"
      ],
      "metadata": {
        "id": "UIv-kRfT0cAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch cross-validation is a valuable technique for evaluating and optimizing linear regression models, ensuring that they generalize well to new data and provide reliable predictions. It helps improve the robustness and generalization ability of the model, leading to more accurate and reliable results in real-world applications."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The application of GridSearchCV on the dataset did not result in a significant improvement in the model's performance scores."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Lasso Regression (L1 Regularization)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso regression, short for Least Absolute Shrinkage and Selection Operator, is a regression technique that performs both variable selection and regularization to improve the predictive performance and interpretability of the model."
      ],
      "metadata": {
        "id": "WwweS-aQHstL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso  = Lasso()\n",
        "\n",
        "# Fit the Algorithm\n",
        "lasso.fit(x_train, y_train)\n",
        "lasso.score(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_las_test = lasso.predict(x_test)\n",
        "y_pred_las_train= reg.predict(x_train)\n",
        "\n",
        "print(f'Training Score : {lasso.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {lasso.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "0KrmIgmN9EoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAE_lasso = mean_absolute_error(10**(y_test),(10**y_pred_las_test))\n",
        "print(\"MAE :\", MAE_lasso)\n",
        "\n",
        "MSE_lasso  = mean_squared_error(10**(y_test), 10**(y_pred_las_test))\n",
        "print(\"MSE :\" , MSE_lasso)\n",
        "\n",
        "RMSE_lasso = np.sqrt(MSE_lasso)\n",
        "print(\"RMSE :\" ,RMSE_lasso)\n",
        "\n",
        "r2_lasso = r2_score(10**(y_test), 10**(y_pred_las_test))\n",
        "print(\"R2 :\" ,r2_lasso)\n",
        "\n",
        "adj_r2_lasso = 1-(1-r2_score(10**(y_test), 10**(y_pred_las_test)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",adj_r2_lasso)"
      ],
      "metadata": {
        "id": "CSrcao649MEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metric_lasso=['MAE','MSE','RMSE','R2']\n",
        "metrics_score_las=[MAE_lasso,MSE_lasso,RMSE_lasso,r2_lasso]\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(metric_lasso,metrics_score_las,color='blue', linewidth = 3,marker='o', markerfacecolor='yellow', markersize=10)\n",
        "plt.gca().set_xlabel('Evaluation Metrics')\n",
        "plt.gca().set_ylabel('Metrics Scores')\n",
        "plt.gca().set_title('Visualization of Evaluation Metrics')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='r2', cv=10)\n",
        "# Fit the Algorithm\n",
        "lasso_regressor.fit(x_train, y_train)\n",
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" r2 score is: \", lasso_regressor.best_score_)\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lasso = lasso_regressor.predict(x_test)\n",
        "\n",
        "#Visualizing Actual vs Prediction values for Lasso\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(10**(y_pred_lasso))\n",
        "plt.plot(10**(np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evl_metrics['Lasso']=[MAE_lasso,MSE_lasso,RMSE_lasso ,r2_lasso,adj_r2_lasso]\n",
        "evl_metrics"
      ],
      "metadata": {
        "id": "bcgA7ss18j5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is a technique used for hyperparameter optimization for the given dataset - linear regression model. GridSearchCV automates the process of hyperparameter tuning by systematically searching through different combinations of hyperparameters and selecting the combination that yields the best performance."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE score reduced compared to the previous model which indicates the model is predicting good on unseen data.\n",
        "\n",
        "The best fit alpha value is found 0.001 and R2 score is 0.98174.\n",
        "The inclusion of additional independent variables may not contribute significantly to the model's predictive power.\n",
        "\n",
        "Overall, the model shows good performance after optimization."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - Ridge Regression (L2 Regularization)"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression, also known as L2 Regularization, is a linear regression technique that incorporates regularization to prevent overfitting and improve the generalization of the model."
      ],
      "metadata": {
        "id": "VvtIzmP3qx_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model 3 Implementation\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge  = Ridge()\n",
        "ridge.fit(x_train, y_train)\n",
        "\n",
        "# Fit the Algorithm\n",
        "ridge.fit(x_train, y_train)\n",
        "ridge.score(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_ridge = ridge.predict(x_test)\n",
        "print(f'Training Score : {ridge.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {ridge.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating Metric scores\n",
        "MAE_ridge = mean_absolute_error(10**(y_test),(10**y_pred_ridge))\n",
        "print(\"MAE :\", MAE_ridge)\n",
        "\n",
        "MSE_ridge  = mean_squared_error(10**(y_test), 10**(y_pred_ridge))\n",
        "print(\"MSE :\" , MSE_ridge)\n",
        "\n",
        "RMSE_ridge = np.sqrt(MSE_ridge)\n",
        "print(\"RMSE :\" ,RMSE_ridge)\n",
        "\n",
        "r2_ridge = r2_score(10**(y_test), 10**(y_pred_ridge))\n",
        "print(\"R2 :\" ,r2_ridge)\n",
        "\n",
        "adj_r2_ridge = 1-(1-r2_score(10**(y_test), 10**(y_pred_ridge)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print(\"Adjusted R2 : \",adj_r2_ridge)"
      ],
      "metadata": {
        "id": "NA3OHuIq-V1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metric_ridge=['MAE','MSE','RMSE','R2']\n",
        "metrics_score_rid=[MAE_ridge,MSE_ridge,RMSE_ridge,r2_ridge]\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(metric_ridge,metrics_score_rid,color='yellow', linewidth = 3,marker='o', markerfacecolor='green', markersize=10)\n",
        "plt.gca().set_xlabel('Evaluation Metrics')\n",
        "plt.gca().set_ylabel('Metrics Scores')\n",
        "plt.gca().set_title('Visualization of Evaluation Metrics')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "#parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "parameters = {'alpha': np.arange(-100,100,0.1)}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='r2', cv=5)\n",
        "\n",
        "# Fit the Algorithm\n",
        "ridge_regressor.fit(x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_r = ridge.predict(x_test)"
      ],
      "metadata": {
        "id": "gpxcVITF-rnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing Actual vs Prediction values for Lasso\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(10**(y_pred_r))\n",
        "plt.plot(10**(np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VAt3zAY38GpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evl_metrics['Ridge']=[MAE_ridge,MSE_ridge,RMSE_ridge,r2_ridge,adj_r2_ridge]\n",
        "evl_metrics"
      ],
      "metadata": {
        "id": "waoQrYPZ9C_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used to search over different values of the regularization parameter alpha for Ridge regression. The best model and its associated hyperparameters are then selected based on cross-validated performance, and the final performance of the best model is evaluated on the test set."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has shown significant improvement compared to previous ones, exhibiting the lowest Mean Absolute Error (MAE) and effectively addressing the overfitting issue encountered with the second model. Therefore, I am leaning towards selecting the Ridge Regression (L2 Regularization) model as the best choice among the three."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - Random Forest"
      ],
      "metadata": {
        "id": "3oSbT2ggchK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Regression is a machine learning technique that utilizes an ensemble of decision trees to make predictions. It's well-suited for regression tasks and offers advantages such as handling non-linear relationships and interactions between features, as well as being robust to overfitting and capable of handling large datasets with high dimensionality."
      ],
      "metadata": {
        "id": "mtHIsE7-5U48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "rfr = RandomForestRegressor()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rfr.fit(x_train, y_train)\n",
        "rfr.score(x_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_rfr_train_pred= rfr.predict(x_train)\n",
        "y_rfr_test_pred= rfr.predict(x_test)\n",
        "\n",
        "print(f'Training Score : {rfr.score(x_train,y_train)}')\n",
        "print(f'Testing Score : {rfr.score(x_test,y_test)}')"
      ],
      "metadata": {
        "id": "SPyd3AFKchK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating Metric Score for train set\n",
        "train_MAE_rf = mean_absolute_error(10**(y_train),(10**y_rfr_train_pred))\n",
        "print(f\"MAE : {train_MAE_rf}\")\n",
        "\n",
        "\n",
        "train_MSE_rf  = mean_squared_error(10**(y_train), 10**(y_rfr_train_pred))\n",
        "print(\"MSE :\" , train_MSE_rf)\n",
        "\n",
        "train_RMSE_rf = np.sqrt(train_MSE_rf)\n",
        "print(\"RMSE :\" ,train_RMSE_rf)\n",
        "\n",
        "train_r2_rf = r2_score(10**(y_train), 10**(y_rfr_train_pred))\n",
        "print(\"R2 :\" ,train_r2_rf)\n",
        "\n",
        "train_adj_r2_rf=1-(1-r2_score(10**(y_train), 10**(y_rfr_train_pred)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adj_r2_rf)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Evaluating Metric Score for test set\n",
        "MAE_rf = mean_absolute_error(10**(y_test),(10**y_rfr_test_pred))\n",
        "print(f\"MAE : {MAE_rf}\")\n",
        "\n",
        "MSE_rf  = mean_squared_error(10**(y_test), 10**(y_rfr_test_pred))\n",
        "print(\"MSE :\" , MSE_rf)\n",
        "\n",
        "RMSE_rf = np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\" ,RMSE_rf)\n",
        "\n",
        "r2_rf = r2_score(10**(y_test), 10**(y_rfr_test_pred))\n",
        "print(\"R2 :\" ,r2_rf)\n",
        "\n",
        "adj_r2_rf=1-(1-r2_score(10**(y_test), 10**(y_rfr_test_pred)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adj_r2_rf)"
      ],
      "metadata": {
        "id": "JwS1I5VDpQZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_reg_list = {'Train Mean Absolute Error':train_MAE_rf,'Train Mean squared Error' : train_MSE_rf,'Train Root Mean squared Error' : train_RMSE_rf,'Train R2 score' : train_r2_rf,'Train Adjusted R2 score' : train_adj_r2_rf,'Mean Absolute Error':MAE_rf,'Mean squared Error' : MSE_rf,'Root Mean squared Error' : RMSE_rf,'R2 score' : r2_rf,'Adjusted R2 score' : adj_r2_rf }\n",
        "evr_metrics = pd.DataFrame.from_dict(RF_reg_list, orient='index').reset_index()\n",
        "evr_metrics = eva_metrics.rename(columns={'index':' Evaluation Metric',0:'Random Forest'})\n",
        "evr_metrics"
      ],
      "metadata": {
        "id": "2WcPN2Oh6Bn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Dm1Hs8avchK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metric_rf=['MAE','MSE','RMSE','R2','Adjusted R2']\n",
        "metrics_score_rf=[MAE_rf,MSE_rf,RMSE_rf,r2_rf,adj_r2_rf]\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(metric_rf,metrics_score_rf,color='yellow', linewidth = 3,marker='o', markerfacecolor='red', markersize=12)\n",
        "plt.gca().set_xlabel('Evaluation Metrics')\n",
        "plt.gca().set_ylabel('Metrics Scores')\n",
        "plt.gca().set_title('Visualization of Evaluation Metrics')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l50lDxJ4chK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.plot(10**((y_rfr_test_pred)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3CI92N_i1TKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "kbWERHvBchK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid_rf = {'n_estimators': [50,80,100,200,300],\n",
        "                 'max_depth': [1,2,6,7,8,9,10,20,30,40],\n",
        "                 'min_samples_split':[10,20,30,40,50,100,150,200],\n",
        "                 'min_samples_leaf': [1,2,8,10,20,40,50]}\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(rfr, param_grid_rf,verbose=2, cv=5, scoring='r2')\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best estimator\n",
        "best_model_rf_rs = random_search.best_estimator_\n",
        "\n",
        "print(best_model_rf_rs)"
      ],
      "metadata": {
        "id": "DROjc-XschK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the model\n",
        "y_rf_rs_pred_train= random_search.predict(x_train)\n",
        "y_rf_rs_pred_test= random_search.predict(x_test)\n",
        "random_search.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "5Gj8MsAb4UTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric Score for train set\n",
        "train_MAE_rf_rs = mean_absolute_error(10**(y_train),(10**y_rf_rs_pred_train))\n",
        "print(f\"MAE : {train_MAE_rf_rs}\")\n",
        "\n",
        "\n",
        "train_MSE_rf_rs  = mean_squared_error(10**(y_train), 10**(y_rf_rs_pred_train))\n",
        "print(\"MSE :\" , train_MSE_rf_rs)\n",
        "\n",
        "train_RMSE_rf_rs = np.sqrt(train_MSE_rf_rs)\n",
        "print(\"RMSE :\" ,train_RMSE_rf_rs)\n",
        "\n",
        "train_r2_rf_rs = r2_score(10**(y_train), 10**(y_rf_rs_pred_train))\n",
        "print(\"R2 :\" ,train_r2_rf_rs)\n",
        "\n",
        "train_adjusted_r2_rf_rs=1-(1-r2_score(10**(y_train), 10**(y_rf_rs_pred_train)))*((x_train.shape[0]-1)/(x_train.shape[0]-x_train.shape[1]-1))\n",
        "print('Adjusted R2:', train_adjusted_r2_rf_rs)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Metric Score for test set\n",
        "MAE_rf_rs = mean_absolute_error(10**(y_test),(10**y_rf_rs_pred_test))\n",
        "print(f\"MAE : {MAE_rf_rs}\")\n",
        "\n",
        "MSE_rf_rs  = mean_squared_error(10**(y_test), 10**(y_rf_rs_pred_test))\n",
        "print(\"MSE :\" , MSE_rf_rs)\n",
        "\n",
        "RMSE_rf_rs = np.sqrt(MSE_rf_rs)\n",
        "print(\"RMSE :\" ,RMSE_rf_rs)\n",
        "\n",
        "r2_rf_rs = r2_score(10**(y_test), 10**(y_rf_rs_pred_test))\n",
        "print(\"R2 :\" ,r2_rf_rs)\n",
        "\n",
        "adjusted_r2_rf_rs=1-(1-r2_score(10**(y_test), 10**(y_rf_rs_pred_test)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "print('Adjusted R2:', adjusted_r2_rf_rs)"
      ],
      "metadata": {
        "id": "hmSwR1w45YR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.plot(10**((y_rf_rs_pred_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TvU_wTb06bK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evl_metrics['Random Forest']=[MAE_rf,MSE_rf,RMSE_rf ,r2_rf,adj_r2_rf]\n",
        "evl_metrics"
      ],
      "metadata": {
        "id": "sYBC62KV4Y9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "wDDbDZcnchK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedSearchCV is a technique used for hyperparameter optimization in machine learning, including for Random Forest models. It efficiently searches through a specified number of random combinations of hyperparameters and selects the combination that yields the best performance. This method saves time compared to exhaustive grid search by randomly sampling hyperparameters from predefined distributions. After fitting the RandomizedSearchCV object to the training data and selecting the best model based on cross-validated performance, the final model's performance is evaluated on the test set to obtain an unbiased estimate of its performance on unseen data."
      ],
      "metadata": {
        "id": "Q3HxvLh4chK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JJTTqOEQchK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From evaluation metric chart, we can conclude that Random Forest holds the lowest Mean Absolute Error(MAE). After applying cross-validation and hyperparameter tuning, the model has shown improvement by effectively addressing the overfitting issue.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LPHjblRYchK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Mean Squared Error (MSE) and Adjusted R-squared (Adj R2) as evaluation metrics positively impacts model performance and decision-making in various ways:\n",
        "\n",
        "Improved Prediction Accuracy: Optimization based on MSE and Adj R2 enhances prediction accuracy, enabling more precise decision-making. This results in better resource allocation and reduced costs associated with errors.\n",
        "\n",
        "Enhanced Model Selection: Comparing models using MSE and Adj R2 helps select the most appropriate model for a specific problem, leading to improved performance and outcomes.\n",
        "\n",
        "In summary, the evaluation metrics offer insights into data, improve decision-making, and ultimately drive better outcomes for businesses."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have selected Random Forest as my final model due to its superior performance in terms of R2 and adjusted R2 values. These metrics indicate that Random Forest is effectively explaining the variance in the target variable while considering all features.\n",
        "\n",
        "Although Ridge Regression also performed well, it showed a lower adjusted R2 on the test data compared to the train data. This suggests that not all features are necessary for Ridge Regression to predict the closing price. However,it deliberately included lag values in the features to capture previous data and patterns.\n",
        "\n",
        "By using cross-validation to estimate the optimal value of alpha in Ridge Regression, we can build a more robust and generalizable model that performs well on new data.\n",
        "\n",
        "Whether Random Forest or Ridge Regression is better depends on the specific requirements of the problem, including the nature of the data, the need for interpretability, and the desired level of performance.Random Forest is a non-linear model that can capture complex relationships between features and the target variable. On the other hand, Ridge Regression is a linear model that assumes a linear relationship between features and the target variable.\n",
        "\n",
        "Overall, Random Forest's ability to capture all features and still predict better than Ridge Regression, along with its lower RMSE and better performance on the test data compared to the train data, makes it the preferred choice for the final model.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have selected Random Forest as my final model due to its superior performance and for model explainability I am using SHAP (Shapley Additive exPlanations) value.\n",
        "\n",
        "SHAP is an improvement of the method for machine learning model explainability study. It is used to calculate the impact of each part of the model on the final result. Shapley's value assumes that we can compute the value of the surplus with or without each analyzed factor. The goal of SHAP is to explain the prediction by computing the contribution of each feature to the final result.\n",
        "\n",
        "![SHAP.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "Mu-VGeZO72ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import shap\n",
        "import graphviz\n",
        "sns.set_style('darkgrid')"
      ],
      "metadata": {
        "id": "P5MNhoLe8ir0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize JavaScript visualizations in notebook environment\n",
        "shap.initjs()\n"
      ],
      "metadata": {
        "id": "hxGVrRtDBnQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = ybank_df.dropna().drop(['Close','Open','High','Low','Month','Year'],axis=1).columns\n",
        "x_shap = x_train\n",
        "# Define a tree explainer for the built model\n",
        "explainer = shap.TreeExplainer(rfr)\n",
        "# obtain shap values for the first row of the test data\n",
        "shap_values = explainer.shap_values(x_shap)\n"
      ],
      "metadata": {
        "id": "V7DJe7lf7WTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "sA0l0M5u8YqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forceplot for values\n",
        "shap.force_plot(explainer.expected_value,shap_values,x_shap)"
      ],
      "metadata": {
        "id": "v-utaObR5utq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.force_plot(explainer.expected_value[0], shap_values[0,:], x_shap[0,:], matplotlib=True,feature_names=feature_names)"
      ],
      "metadata": {
        "id": "OlC27pmH6Eyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The starting point for the analysis is the average result in our data set. SHAP checks how the deviation from the average was impacted by each variable. The prediction starts from the baseline. The baseline for Shapley values is the average of all predictions. Each feature value is a force that either increases or decreases the prediction. Each feature contributing to pushing the model output from the base value to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue."
      ],
      "metadata": {
        "id": "gyeervkE9J6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary goal is to forecast the monthly closing price of Yes Bank's stock, considering the impact of the 2018 fraud case, using historical monthly stock price data. The dataset includes metrics such as opening, closing, highest, and lowest prices for each month since the establishment of Yes Bank.\n",
        "\n",
        "Due to fluctuations over time, especially significant events like the 2018 fraud, variables like 'Close' and 'Open', 'High', 'Low' may exhibit positive skewness. Applying log transformation has led to a more symmetrical distribution, improving the accuracy of predictions. The closing price shows high correlation with other variables.\n",
        "\n",
        "The log transformation effectively reduced outliers in the dataset, contributing to a more robust model. However, completely removing outliers from a small dataset is generally not advisable.\n",
        "\n",
        "I aggregated 'Open', 'High', and 'Low' features by taking their mean and engineered additional features using lag values to capture temporal trends and patterns, including potential effects of the fraud case.\n",
        "\n",
        "Four machine learning models were constructed - Linear Regression, Lasso Regression, Ridge Regression, and Random Forest. Evaluation metrics provide insights into data, aiding decision-making for better outcomes. Cross-validation was utilized to estimate optimal values, ensuring robust and generalizable models for new data.\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) is an advanced method for studying model explainability in machine learning. It calculates the contribution of each feature to the final prediction, providing insights into how individual components of the model influence the overall outcome.\n",
        "\n",
        "Given the dataset and features, our model demonstrates strong performance across all data points, showcasing its robustness in capturing patterns and trends within the data. This indicates that the model effectively generalizes to unseen instances and maintains consistent accuracy in its predictions. Such reliability underscores the efficacy of our model in addressing the complexities inherent in the dataset, thereby instilling confidence in its predictive capabilities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}